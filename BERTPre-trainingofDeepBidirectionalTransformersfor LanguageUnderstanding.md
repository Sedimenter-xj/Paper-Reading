# BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding

 We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a;Rad fordet al., 2018),BERT is designed to pre train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks,such as question answering and language inference,without substantial taskspecific architecture modifications.

BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5%(7.7% point absolute improvement), MultiNLI accuracy to 86.7%(4.6%absolute improvement),SQuADv1.1 question answer ingTest F1 to 93.2 (1.5point absolute im provement) and SQuADv2.0 Test F1 to 83.1 (5.1 point absolute improvement).

â€œBidirectionalâ€ çš„æ„æ€æ˜¯â€œåŒå‘çš„â€ã€‚åœ¨BERTé‡Œï¼Œâ€œbidirectionalâ€æŒ‡çš„æ˜¯æ¨¡å‹åœ¨ç†è§£è¯­è¨€æ—¶ï¼Œä¼šåŒæ—¶è€ƒè™‘ä¸€å¥è¯ä¸­ä¸€ä¸ªè¯å·¦è¾¹å’Œå³è¾¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åªä»å·¦å¾€å³æˆ–è€…åªä»å³å¾€å·¦çœ‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹æ˜¯åŒå‘åœ°ç†è§£å¥å­é‡Œçš„æ¯ä¸ªè¯ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°æ›´å¤šä¸°å¯Œçš„è¯­ä¹‰å…³ç³»ã€‚

ä¸¾ä¸ªç®€å•ä¾‹å­ï¼š
 å¥å­â€œä»–**åœ¨é“¶è¡Œ**å­˜äº†é’±â€ï¼Œæ¨¡å‹ä¼šåŒæ—¶å…³æ³¨â€œé“¶è¡Œâ€å·¦è¾¹çš„è¯â€œä»–â€å’Œå³è¾¹çš„è¯â€œå­˜äº†é’±â€ï¼Œä»è€Œæ›´å‡†ç¡®ç†è§£â€œé“¶è¡Œâ€æ˜¯æŒ‡é‡‘èæœºæ„ï¼Œè€Œä¸æ˜¯æ²³å²¸ã€‚

**deep bidirectional representations:**

- bidirectional:æ¨¡å‹ä¸ä»…çœ‹ä¸€ä¸ªè¯å·¦è¾¹çš„å†…å®¹ï¼Œä¹Ÿçœ‹å®ƒå³è¾¹çš„å†…å®¹ï¼Œç†è§£è¿™ä¸ªè¯åœ¨å¥å­é‡Œçš„å®Œæ•´è¯­ä¹‰ç¯å¢ƒ
- deep:è¿™ç§ç†è§£ä¸æ˜¯åªåœ¨è¡¨é¢åšä¸€æ¬¡ï¼Œè€Œæ˜¯åœ¨æ¨¡å‹çš„å¤šä¸ªéšè—å±‚ï¼ˆå¾ˆå¤šå±‚ç¥ç»ç½‘ç»œå±‚ï¼‰ä¸­åå¤è¿›è¡Œï¼Œè¿™æ ·èƒ½æ•æ‰æ›´å¤æ‚ã€æ›´æŠ½è±¡çš„è¯­è¨€ç‰¹å¾ã€‚

**Question Answeringï¼ˆé—®ç­”ï¼‰**ï¼šæŒ‡çš„æ˜¯æ¨¡å‹æ ¹æ®ç»™å®šçš„æ–‡æœ¬ï¼ˆæ¯”å¦‚ä¸€æ®µæ–‡ç« æˆ–çŸ¥è¯†åº“ï¼‰æ¥å›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚æ¯”å¦‚ä½ ç»™æˆ‘ä¸€æ®µæ–‡ç« ï¼Œç„¶åé—®æˆ‘â€œè¿™ç¯‡æ–‡ç« ä¸»è¦è®²ä»€ä¹ˆï¼Ÿâ€ï¼Œæ¨¡å‹å°±è¦ä»æ–‡ç« ä¸­æ‰¾å‡ºç­”æ¡ˆã€‚

**Language Inferenceï¼ˆè¯­è¨€æ¨ç†ï¼‰**ï¼Œä¹Ÿå«â€œè‡ªç„¶è¯­è¨€æ¨æ–­â€ï¼ˆNatural Language Inferenceï¼ŒNLIï¼‰ï¼šæŒ‡çš„æ˜¯åˆ¤æ–­ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼Œæ¯”å¦‚åˆ¤æ–­ç¬¬äºŒå¥è¯æ˜¯ä¸æ˜¯ç¬¬ä¸€å¥è¯çš„**è•´å«**ã€**çŸ›ç›¾**ï¼Œè¿˜æ˜¯**ä¸­ç«‹**ã€‚æ¯”å¦‚å¥å­Aæ˜¯â€œæ‰€æœ‰çš„çŒ«éƒ½æ˜¯åŠ¨ç‰©â€ï¼Œå¥å­Bæ˜¯â€œè¿™åªçŒ«æ˜¯ä¸€åªåŠ¨ç‰©â€ï¼Œæ¨¡å‹è¦åˆ¤æ–­Bæ˜¯å¦ç”±Aè•´å«ã€‚å…¸å‹æ•°æ®é›†æœ‰MultiNLIã€‚

**GLUEï¼ˆGeneral Language Understanding Evaluationï¼‰**

- ä¸€ä¸ªç”±å¤šä¸ªå­ä»»åŠ¡ç»„æˆçš„æµ‹è¯•é›†åˆï¼Œç”¨æ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚
- æ–‡æœ¬åˆ†ç±»ã€å¥å­é…å¯¹ã€æƒ…æ„Ÿåˆ†æã€è‡ªç„¶è¯­è¨€æ¨ç†ç­‰ã€‚
- å¦‚æœä¸€ä¸ªæ¨¡å‹åœ¨GLUEå¾—åˆ†é«˜ï¼Œè¯´æ˜å®ƒåœ¨å¾ˆå¤šç§è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å¾—å¾ˆå¥½ã€‚

**MultiNLIï¼ˆMulti-Genre Natural Language Inferenceï¼‰**

- ä¸€ä¸ªä¸“é—¨ç”¨äº**è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰**çš„ä»»åŠ¡ã€‚
- **ä»»åŠ¡ç›®æ ‡**ï¼šç»™å®šä¸€å¯¹å¥å­ï¼ˆå‰æ + å‡è®¾ï¼‰ï¼Œæ¨¡å‹è¦åˆ¤æ–­å‡è®¾æ˜¯å¦æ˜¯ï¼š
  - **è•´å«**ï¼ˆentailmentï¼‰
  - **çŸ›ç›¾**ï¼ˆcontradictionï¼‰
  - **ä¸­ç«‹**ï¼ˆneutralï¼‰

**SQuAD v1.1ï¼ˆStanford Question Answering Datasetï¼‰**

- ä¸€ä¸ªåŸºäºç»´åŸºç™¾ç§‘çš„é—®ç­”æ•°æ®é›†ã€‚
- **ä»»åŠ¡ç›®æ ‡**ï¼šæ¨¡å‹é˜…è¯»ä¸€æ®µæ–‡ç« ï¼Œç„¶åå›ç­”ä¸€ä¸ªåŸºäºæ–‡ç« çš„é—®é¢˜ï¼Œç­”æ¡ˆå¿…é¡»æ˜¯**åŸæ–‡ä¸­çš„ä¸€æ®µè¿ç»­æ–‡æœ¬**ã€‚

**SQuAD v2.0**

- **æ˜¯åœ¨v1.1åŸºç¡€ä¸Šçš„å‡çº§ç‰ˆ**
- **æ–°åŠ å…¥çš„æŒ‘æˆ˜**ï¼šå¢åŠ äº†**æ— ç­”æ¡ˆçš„é—®é¢˜**ï¼Œå³é—®é¢˜æ ¹æœ¬æ— æ³•ä»æ–‡ä¸­å›ç­”ã€‚
- **æ¨¡å‹ä»»åŠ¡**ï¼šä¸ä»…è¦å›ç­”æ­£ç¡®çš„é—®é¢˜ï¼Œè¿˜è¦å­¦ä¼šåˆ¤æ–­æŸäº›é—®é¢˜æ˜¯**æ²¡æœ‰ç­”æ¡ˆçš„**ã€‚

 Language model pre-training has been shown to be effective for improving many natural language processing tasks(DaiandLe,2015;Petersetal., 2018a;Radfordetal.,2018;HowardandRuder, 2018).These include sentence-level tasks such as natural language inference(Bowmanetal.,2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett,2005),which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (TjongKim Sangand DeMeulder,2003;Rajpurkaretal.,2016).

### Fine-grained output:

### åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œå®ƒé€šå¸¸æŒ‡ï¼š

- **ä¸æ˜¯ä¸€å¥è¯ä¸€ä¸ªç»“æœ**ï¼Œè€Œæ˜¯**å¯¹å¥å­ä¸­çš„æ¯ä¸ªè¯æˆ–ç‰‡æ®µåšåˆ¤æ–­**ã€‚
- è¾“å‡ºçš„ç»“æœæ›´â€œç²¾ç»†â€ï¼Œå¯ä»¥é€ä¸ªè¯åœ°æ ‡æ³¨æˆ–å¤„ç†ã€‚

------

### âœ… ä¸¾å‡ ä¸ªä¾‹å­å¸®ä½ ç†è§£ï¼š

#### 1. **å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰**ï¼š

- è¾“å…¥å¥å­ï¼š**â€œæ¯”å°”Â·ç›–èŒ¨æ˜¯å¾®è½¯çš„åˆ›å§‹äººã€‚â€**
- æ¨¡å‹çš„ fine-grained è¾“å‡ºï¼š
  - â€œæ¯”å°”Â·ç›–èŒ¨â€ â†’ äººåï¼ˆPersonï¼‰
  - â€œå¾®è½¯â€ â†’ ç»„ç»‡åï¼ˆOrganizationï¼‰

ğŸ”¹ æ¨¡å‹éœ€è¦**å¯¹æ¯ä¸ªè¯ä½œå‡ºç»†è‡´åˆ†ç±»**ï¼Œè¿™å°±å«â€œç»†ç²’åº¦è¾“å‡ºâ€ã€‚

------

#### 2. **é—®ç­”ä»»åŠ¡ï¼ˆå¦‚SQuADï¼‰**ï¼š

- æé—®ï¼š**â€œè°åˆ›å»ºäº†å¾®è½¯ï¼Ÿâ€**
- ä¸Šä¸‹æ–‡ï¼š**â€œæ¯”å°”Â·ç›–èŒ¨å’Œä¿ç½—Â·è‰¾ä¼¦åœ¨1975å¹´åˆ›å»ºäº†å¾®è½¯ã€‚â€**
- fine-grained è¾“å‡ºï¼š**â€œæ¯”å°”Â·ç›–èŒ¨å’Œä¿ç½—Â·è‰¾ä¼¦â€**

ğŸ”¹ æ¨¡å‹ä¸ä»…è¦æ‰¾å‡ºå“ªå¥è¯æœ‰ç­”æ¡ˆï¼Œè¿˜è¦**ç²¾ç¡®æå–å‡ºå‡ ä¸ªè¯**ä½œä¸ºç­”æ¡ˆã€‚

There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters etal.,2018a),uses task-specific architectures that include the pre-trained representations as additional features.The fine-tuning approach,such as the Generative Pre-trained Transformer (OpenAI GPT) (Radfordetal.,2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.The two approaches share the same objective function during pre-training,where they use unidirectional language models to learn general language representations.

### Downstream tasks:

åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€BERTï¼‰ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè®­ç»ƒä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œè®©å®ƒå­¦ä¼šè¯­è¨€çš„åŸºæœ¬è§„å¾‹ã€‚è¿™å«åš **é¢„è®­ç»ƒï¼ˆpretrainingï¼‰**ã€‚

ä¹‹åï¼Œæˆ‘ä»¬ä¼šå°†è¿™ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨äºæ›´å…·ä½“çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š

- æ–‡æœ¬åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰
- å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
- é—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰
- æ–‡æœ¬ç”Ÿæˆ
- æœºå™¨ç¿»è¯‘
- æ‘˜è¦ç”Ÿæˆ

è¿™äº›æ›´å…·ä½“ã€ç›®æ ‡æ˜ç¡®çš„ä»»åŠ¡ï¼Œå°±è¢«ç§°ä¸º **ä¸‹æ¸¸ä»»åŠ¡**ï¼ˆdownstream tasksï¼‰ã€‚

We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional,and this limits the choice of architectures that can be used during pre-training. For example,in OpenAI GPT,the authors use a left-to right architecture,where every token can only attend to previous tokens in the self-attention layers of the Transformer(Vaswanietal.,2017).Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine tuning based approaches totoken-level tasks such as question answering,where it is crucial to incorporate context from both directions.

### self-attention layers

**â€œè‡ªæ³¨æ„åŠ›å±‚â€** æˆ– **â€œè‡ªæ³¨æ„åŠ›æœºåˆ¶å±‚â€**ã€‚

å®ƒæŒ‡çš„æ˜¯Transformerç»“æ„ä¸­ç”¨äºè®¡ç®—åºåˆ—å†…å„ä¸ªä½ç½®ä¹‹é—´å…³ç³»çš„å±‚çº§ï¼Œé€šè¿‡â€œæ³¨æ„åŠ›â€æœºåˆ¶è®©æ¨¡å‹å…³æ³¨è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„ä¿¡æ¯ã€‚

å‡è®¾ä¸€å¥è¯æ˜¯ï¼šâ€œæˆ‘ å–œæ¬¢ åƒ è‹¹æœâ€ã€‚

åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œæ¯ä¸ªè¯éƒ½ä¼šçœ‹â€œè‡ªå·±â€å’Œå¥å­é‡Œå…¶ä»–è¯çš„å…³ç³»ï¼Œå†³å®šè‡ªå·±åº”è¯¥å…³æ³¨å“ªäº›è¯çš„ä¿¡æ¯ã€‚

æ¯”å¦‚ï¼Œè¯â€œåƒâ€åœ¨è®¡ç®—å®ƒçš„è¡¨ç¤ºæ—¶ï¼Œä¼šâ€œæ³¨æ„â€åˆ°â€œè‹¹æœâ€è¿™ä¸ªè¯ï¼Œå› ä¸ºâ€œåƒâ€å’Œâ€œè‹¹æœâ€ä¹‹é—´æœ‰å¼ºå…³è”ï¼›åŒæ—¶å®ƒä¹Ÿä¼šå‚è€ƒâ€œæˆ‘â€å’Œâ€œå–œæ¬¢â€çš„ä¿¡æ¯ï¼Œä½†æƒé‡å¯èƒ½æ²¡é‚£ä¹ˆå¤§ã€‚

å…·ä½“æ¥è¯´ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šä¸ºæ¯ä¸ªè¯è®¡ç®—ä¸€ä¸ªæƒé‡åˆ†å¸ƒï¼Œè¡¨ç¤ºè¿™ä¸ªè¯å¯¹å¥ä¸­å…¶ä»–è¯çš„â€œå…³æ³¨åº¦â€ï¼Œå†æ ¹æ®è¿™äº›æƒé‡ç»¼åˆå…¶ä»–è¯çš„ä¿¡æ¯ï¼Œå½¢æˆæ›´æ–°åçš„è¯å‘é‡ã€‚

â€œè¯å‘é‡â€æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é‡Œéå¸¸é‡è¦çš„ä¸€ä¸ªæ¦‚å¿µï¼Œç®€å•æ¥è¯´å°±æ˜¯æŠŠ**è¯è½¬æ¢æˆæ•°å­—è¡¨ç¤º**çš„ä¸€ç§æ–¹æ³•ã€‚

å…·ä½“è§£é‡Šï¼š

- è®¡ç®—æœºä¸èƒ½ç›´æ¥ç†è§£æ–‡å­—ï¼Œå®ƒåªèƒ½å¤„ç†æ•°å­—ã€‚
- è¯å‘é‡å°±æ˜¯æŠŠæ¯ä¸ªè¯è½¬æ¢æˆä¸€ä¸ª**å›ºå®šé•¿åº¦çš„æ•°å­—æ•°ç»„ï¼ˆå‘é‡ï¼‰**ï¼Œè¿™æ ·è®¡ç®—æœºæ‰èƒ½å¤„ç†å’Œåˆ†æè¯­è¨€ã€‚
- è¿™ä¸ªå‘é‡ä¸ä»…ä»…æ˜¯éšæœºæ•°å­—ï¼Œè€Œæ˜¯ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿåæ˜ è¯çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚
- æ¯”å¦‚ï¼Œâ€œçŒ«â€å’Œâ€œç‹—â€çš„è¯å‘é‡åœ¨æ•°å­¦ç©ºé—´ä¸­ä¼šæ¯”è¾ƒæ¥è¿‘ï¼Œå› ä¸ºå®ƒä»¬è¯­ä¹‰ç›¸ä¼¼ï¼›è€Œâ€œçŒ«â€å’Œâ€œæ±½è½¦â€çš„è¯å‘é‡åˆ™ç›¸è·è¾ƒè¿œã€‚

ä¸¾ä¸ªä¾‹å­ï¼š

â€œè‹¹æœâ€çš„è¯å‘é‡å¯èƒ½æ˜¯è¿™æ ·çš„ä¸€ä¸²æ•°å­—ï¼ˆè¿™é‡Œåªæ˜¯ä¸¾ä¾‹ï¼‰ï¼š
 [0.12, -0.05, 0.33, 0.87, ...]

â€œé¦™è•‰â€çš„è¯å‘é‡ä¹Ÿæœ‰ç±»ä¼¼çš„æ•°å­—ï¼Œä½†ä¼šå’Œâ€œè‹¹æœâ€æ›´æ¥è¿‘ï¼Œè€Œå’Œâ€œæ±½è½¦â€å·®åˆ«æ›´å¤§ã€‚

è¯å‘é‡çš„å‡ºç°è®©æœºå™¨èƒ½å¤Ÿâ€œç†è§£â€è¯ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œè¿›è€Œå®Œæˆç¿»è¯‘ã€é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ã€‚

### fine tuning

å…·ä½“æ¥è¯´ï¼š

- å¾®è°ƒæ˜¯æŒ‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨ç‰¹å®šä»»åŠ¡çš„å°è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œè¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ï¼Œè®©æ¨¡å‹æ›´å¥½åœ°é€‚åº”è¿™ä¸ªå…·ä½“ä»»åŠ¡ã€‚
- ä¾‹å¦‚ï¼Œå…ˆç”¨å¤§è§„æ¨¡æ— ç›‘ç£æ•°æ®è®­ç»ƒä¸€ä¸ªé€šç”¨è¯­è¨€æ¨¡å‹ï¼ˆé¢„è®­ç»ƒï¼‰ï¼Œç„¶åç”¨é—®ç­”æ•°æ®å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å®ƒæ›´æ“…é•¿é—®ç­”ä»»åŠ¡ã€‚

å¾®è°ƒçš„å¥½å¤„æ˜¯ï¼š

- åˆ©ç”¨é¢„è®­ç»ƒçš„å¼ºå¤§çŸ¥è¯†å’Œè¡¨ç¤ºèƒ½åŠ›ï¼Œ
- å¿«é€Ÿé€‚åº”å…·ä½“ä»»åŠ¡ï¼Œé¿å…ä»é›¶å¼€å§‹è®­ç»ƒï¼ŒèŠ‚çœæ—¶é—´å’Œèµ„æºã€‚

 In this paper,we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.BERT alleviates the previously mentioned unidirectionality constraint by using a â€œmasked language modelâ€(MLM)pre-training objective, inspired by theCloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to them asked language model,we also use  a â€œnext sentence predictionâ€ task that jointly pretrains text-pair representations.The contributions of our paper are as follows:

**â€œä¸ºä»€ä¹ˆè¦ mask ä¸€äº›è¯ï¼Ÿç„¶ååˆè®©æ¨¡å‹å»é¢„æµ‹å®ƒä»¬ï¼Ÿè¿™æ˜¯ä¸ºäº†è®­ç»ƒä»€ä¹ˆèƒ½åŠ›ï¼Ÿâ€**

------

### âœ… ç®€å•å›ç­”ï¼š

è¿™æ˜¯åœ¨è®­ç»ƒæ¨¡å‹å­¦ä¼š**ç†è§£ä¸Šä¸‹æ–‡ï¼Œå¹¶æ•æ‰è¯­è¨€ä¸­çš„è¯­ä¹‰å’Œè¯­æ³•ä¿¡æ¯**ã€‚

------

### âœ… å…·ä½“è§£é‡Šï¼š

1. **BERT æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹**ï¼Œç›®æ ‡ä¸æ˜¯ä¸€å¼€å§‹å°±åšåˆ†ç±»ã€é—®ç­”ã€ç¿»è¯‘ï¼Œè€Œæ˜¯å…ˆå­¦ä¼šâ€œçœ‹æ‡‚è¯­è¨€â€ã€‚
2. æ‰€ä»¥å®ƒç”¨äº†ä¸€ç§ç±»ä¼¼**â€œå¡«ç©ºé¢˜â€**çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ç†è§£è¯­è¨€ç»“æ„ï¼š
   - æ¯”å¦‚åŸå¥æ˜¯ï¼š
      `The quick brown fox jumps over the lazy dog.`
   - è®­ç»ƒæ—¶å®ƒä¼šéšæœºé®ä½ä¸€äº›è¯ï¼Œæ¯”å¦‚ï¼š
      `The quick [MASK] fox jumps over the [MASK] dog.`
   - ç„¶åæ¨¡å‹çš„ä»»åŠ¡æ˜¯ï¼š**ä»…é å‰åä¸Šä¸‹æ–‡çŒœå‡º `[MASK]` æ˜¯ä»€ä¹ˆè¯**ã€‚
     - ç¬¬ä¸€ä¸ª [MASK] åº”è¯¥çŒœå‡ºæ˜¯ `brown`
     - ç¬¬äºŒä¸ª [MASK] åº”è¯¥çŒœå‡ºæ˜¯ `lazy`
3. **â€œé¢„æµ‹å•è¯çš„è¯æ±‡ IDâ€** å…¶å®å°±ç­‰äºé¢„æµ‹å‡ºå®ƒåŸæœ¬æ˜¯å“ªä¸ªè¯ï¼ˆè¯æ±‡è¡¨ä¸­çš„ç¼–å·ï¼‰ï¼Œæœ¬è´¨æ˜¯ä¸€ä¸ª**åˆ†ç±»é—®é¢˜**ã€‚

------

### âœ… ä¸ºä»€ä¹ˆè¿™æ ·è®­ç»ƒæœ‰ç”¨ï¼Ÿ

å› ä¸ºï¼š

- å¦‚æœä¸€ä¸ªæ¨¡å‹èƒ½ä»…é å‰åè¯ï¼Œæ­£ç¡®çŒœå‡ºè¢«é®ä½çš„è¯ï¼Œé‚£è¯´æ˜å®ƒçœŸçš„**ç†è§£äº†è¯­è¨€ä¸­çš„è¯åºã€è¯­ä¹‰ã€æ­é…**ç­‰çŸ¥è¯†ã€‚
- å®ƒè®­ç»ƒå‡ºçš„è¯å‘é‡ã€å¥å­è¡¨ç¤ºå°±éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ¯”å¦‚æƒ…æ„Ÿåˆ†æã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚

------

### âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> Mask ä¸€äº›è¯å†è®©æ¨¡å‹é¢„æµ‹ï¼Œæ˜¯ä¸ºäº†è®©æ¨¡å‹å­¦ä¼šâ€œå¡«ç©ºâ€ï¼Œä»è€Œå­¦ä¼šç†è§£è¯­è¨€çš„ä¸Šä¸‹æ–‡ã€è¯ä¹‰å’Œè¯­æ³•è§„å¾‹ â€”â€” è¿™æ˜¯ BERT é¢„è®­ç»ƒçš„æ ¸å¿ƒç›®æ ‡ã€‚

### âœ… ä»€ä¹ˆæ˜¯ text-pair representationsï¼Ÿ

å®ƒæŒ‡çš„æ˜¯ï¼š
 ğŸ‘‰ æŠŠä¸€å¯¹æ–‡æœ¬ï¼ˆæ¯”å¦‚ä¸¤ä¸ªå¥å­ã€ä¸€ä¸ªé—®é¢˜+ä¸€ä¸ªå›ç­”ï¼‰**ä½œä¸ºä¸€ç»„è¾“å…¥**ï¼Œé€šè¿‡æ¨¡å‹ç¼–ç ï¼Œå¾—åˆ°å®ƒä»¬**è”åˆçš„è¯­ä¹‰è¡¨ç¤º**ã€‚

------

### âœ… ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

å› ä¸ºå¾ˆå¤šä»»åŠ¡æ¶‰åŠä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä¾‹å¦‚ï¼š

| ä»»åŠ¡ç±»å‹               | æ–‡æœ¬å¯¹ç¤ºä¾‹           | æ¨¡å‹ç›®çš„                   |
| ---------------------- | -------------------- | -------------------------- |
| **è‡ªç„¶è¯­è¨€æ¨ç† (NLI)** | Premise + Hypothesis | åˆ¤æ–­æ˜¯å¦è•´å« / çŸ›ç›¾ / ä¸­æ€§ |
| **é—®ç­” (QA)**          | é—®é¢˜ + æ®µè½          | æ‰¾å‡ºæ®µè½ä¸­èƒ½å›ç­”é—®é¢˜çš„éƒ¨åˆ† |
| **å¥å­ç›¸ä¼¼åº¦**         | å¥å­1 + å¥å­2        | åˆ¤æ–­ä¸¤ä¸ªå¥å­è¯­ä¹‰ç›¸ä¼¼ç¨‹åº¦   |
| **å¥å­é…å¯¹**           | è¯„è®º + å›å¤          | åˆ¤æ–­æ˜¯å¦æ˜¯åˆç†çš„ä¸€å¯¹       |



è¿™äº›ä»»åŠ¡éƒ½è¦æ±‚æ¨¡å‹ç†è§£ä¸¤ä¸ªå¥å­ä¹‹é—´çš„**å…³ç³»**ï¼Œè€Œä¸ä»…ä»…æ˜¯ç‹¬ç«‹ç†è§£æŸä¸€ä¸ªå¥å­ã€‚

------

### âœ… BERT æ€ä¹ˆå¤„ç† text-pairï¼Ÿ

BERT çš„è¾“å…¥æ ¼å¼æ”¯æŒå¥å­å¯¹ç»“æ„ï¼š

```
css


å¤åˆ¶ç¼–è¾‘
[CLS] å¥å­A [SEP] å¥å­B [SEP]
```

- `[CLS]`ï¼šæ•´ä¸ªå¥å­å¯¹çš„â€œæ‘˜è¦â€è¡¨ç¤ºï¼Œç”¨æ¥åšåˆ†ç±»
- `[SEP]`ï¼šåˆ†éš”ä¸¤ä¸ªå¥å­

BERT é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æŠŠä¸¤ä¸ªå¥å­æ··åˆå»ºæ¨¡ï¼Œæœ€åå¾—åˆ°è¿™å¯¹æ–‡æœ¬çš„â€œè”åˆè¡¨ç¤ºâ€ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„ **text-pair representation**ã€‚

------

### âœ… æ€»ç»“ï¼š

> â€œText-pair representationsâ€ å°±æ˜¯æ¨¡å‹å¯¹ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´è¯­ä¹‰å…³ç³»çš„å»ºæ¨¡å’Œç¼–ç æ–¹å¼ï¼Œæ˜¯å®ç°è‡ªç„¶è¯­è¨€æ¨ç†ã€é—®ç­”ã€å¥å­åŒ¹é…ç­‰ä»»åŠ¡çš„æ ¸å¿ƒè¡¨ç¤ºæ–¹å¼ã€‚

We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radfordetal.(2018),which uses unidirectional language models for pre-training,BERT uses masked language models to enable pretrained deep bidirectional representations.This is also incontrast to Peters et al.(2018a),which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.

We show that pre-trained representations reduce the need for many heavily-engineered task specific architectures. BERT is the first fine tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks,outperforming many task-specific architectures.

